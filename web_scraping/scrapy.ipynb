{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scrapy\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# course : https://www.youtube.com/watch?v=mBoX_JCKZTE\n",
    "\n",
    "# to start scraping project, go to command line : \n",
    "    # scrapy startproject project_name\n",
    "        # you will see that 'spider' folder and many other files are created in the directory. \n",
    "        # 'spider' folder contains files for spiders(scrapy crawlers)\n",
    "        # pipelines.py for pipelines, etc\n",
    "        # settings.py for settings of the crawler. you can change however you want\n",
    "        # scrapy.cfg is a configuration file. \n",
    "\n",
    "# navigate to 'spiders' folder in command line then, we create a spider using the following command : \n",
    "    # scrapy genspider spider_name url_to_scrape\n",
    "# now, you will see a spider_name.py file inside 'spiders' directory\n",
    "\n",
    "\n",
    "# if you want to use ipthon shell instead of default scrapy shell, you can do the following:\n",
    "#  install ipython(pip install ipython) and in ../scrapy.cfg file, you need to add set shell as 'ipython'[shell = ipython ---> add this line in scrapy.cfg file] \n",
    "# you can use scrapy shell to run commands before impementing it in the script.(command to start the shell : scrapy shell)\n",
    "\n",
    "# to start crawling : \n",
    "    # scrapy crawl spider_name\n",
    "\n",
    "\n",
    "# to initialize items, have to import the class that is declared in the 'items.py' file in the spider script.\n",
    "# items.py has a class that has variables to hold the scraped values. \n",
    "\n",
    "# we can use pipelines(pipelines.py) for preprocessing the data. you will also need to uncomment the 'ITEM_PIPELINES' variable from settings.py file and check if the pipeline name inside the variable matches the class name of your pipeline otherwise you will have errors.\n",
    "\n",
    "\n",
    "# saving the scraped data:\n",
    "    # via command line  : scrapy crawl spider_name -O file_name.extension -> rewrite the file, if already exists. but if :     scrapy crawl spider_name -o file_name.extension -> append the data, if file already exists\n",
    "    # via feed settings : add the following lines in the 'settings.py' file : \n",
    "            # FEEDS = {\n",
    "            #           'filename.extension' : {'format' : 'extension'}   \n",
    "            # }\n",
    "                        # filename.extension will be created with the scraped data.\n",
    "\n",
    "            \"\"\"   \n",
    "                    you can also provide custom setting in the spider script file itself. just add 'custom_settings' dictionary containing all the settings.\n",
    "                    for example : \n",
    "                        custrom_settings = {\n",
    "                            'FEEDS' : {'filename.extension' : {'format' : 'extension', 'overwrite' : True/False}}   #if set True, then overwrite the settings in settings.py file if this setting is already present in the file.\n",
    "                        }   \n",
    "            \"\"\"\n",
    "    # saving the data into databases from pipelines.py file : see pipelines.py file.\n",
    "\n",
    "\n",
    "\n",
    "    # fake user agents and request headers : many websites try to block the bots by seeing our request headers(we can see out request headers under 'network' section inside inspect elements in our browser. go to network section and reload the webpage and click on the url and scroll down you will see the request headers inside headers section and you will see the user agent at the bottom of the request headers. you can decode the user agent by pasting the user agent value in https://useragentstring.com/)\n",
    "        # setting our own user agent value : \n",
    "            # go to settings.py and enter :\n",
    "                # USER_AGENT = 'valid useragent string' example : USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "            # but this method is useless as it doesnt prevent us from getting bolocked by website because it doesnt change the user agent value.\n",
    "\n",
    "        # so in order to avoid getting blocked, one thing we can do is change the request headers and user agents at every request / at every 'n' requests.\n",
    "            # first, create a list of fake user agents in your spider script.(you can download a list of fake user agents from internet)\n",
    "            # then, at wherever you are making a request , you need to specify the user agent value. for example, see 'adv_spider.py' spider script.\n",
    "\n",
    "        # getting thousands of random user agents and request headers from https://scrapeops.io/ to scrape massive websites like amazon.com, etc..\n",
    "                # login credentials : username : 537fm7dn@duck.com\n",
    "                                    # password : p@$$w0rd\n",
    "                                    # api key : 57d1e6ca-6e1f-4cd2-8f53-f2ce027ad9f2\n",
    "                # for fake user agents : we have created a class(getuseragents) in middlewares.py. to enable this middleware you have to uncomment the 'DOWNLOADER_MIDDLEWARES' variable in 'settings.py' and add your class in the variable. (for example see 'adv_spider_sub1.py' spider script and see 'getuseragents' class in 'middlewares.py')\n",
    "\n",
    "                # for fake response headers : we have created a class(getrequestheaders) in middlewares.py. to enable this middleware you have to uncomment the 'DOWNLOADER_MIDDLEWARES' variable in 'settings.py' and add your class in the variable. (for example see 'adv_spider_sub1.py' spider script and see 'getrequestheaders' class in 'middlewares.py')\n",
    "                        # you can also integrate the scrapeops api directly into the spider script also where we can spin up new fake browser headers and user-agents during each iteration.\n",
    "\n",
    "        # the website can also detect the bot if we use the same ip address to scrape. so we can use proxys to avoid getting blocked\n",
    "                # install scrapy-rotating-proxies(pip install scrapy-rotating-proxies)- middleware for changing proxies while scraping\n",
    "                #first, we have prepared a list of proxies in settings.py. make  sure that the variable name where you have stored the proxies is . these proxies are taken from 'https://free-proxy-list.net/' and 'https://geonode.com/free-proxy-list'.\n",
    "                # then, go ahead and add these middlewares in the 'DOWNLOADER_MIDDLEWARES' variable\n",
    "                        # \"rotating_proxies.middlewares.RotatringProxyMiddleware\" : n,  \n",
    "                        # \"rotating_proxies.middlewares.BanDetectionMiddleware\" : n+1      here, n => integer\n",
    "                # if you have saved list of proxies in a file, then save the path of the file in 'ROTATING_PROXY_LIST_PATH' variable in 'settings.py' file\n",
    "\n",
    "                # NOTE :  the main disadvantage of the above method is that these free proxies are used my others also so they will be very slow. you can get your own deticated proxies from 'https://smartproxy.com/'. then you can either implement it directly into your spider script by passing \"meta = {'proxy' : url}\" in scrapy.Request() or response.follow(). \n",
    "                    # example : \n",
    "                            # yield scrapy.Request(relative_url, callback = self.parse_book_page, meta = {'proxy' : url})\n",
    "\n",
    "                    #  or else you can also create a class in middleware and change your proxies there.\n",
    "                        # example : \n",
    "                                # first, all the following lines in the 'settings.py' file\n",
    "                                    ## settings.py\n",
    "                                        # PROXY_USER = 'username'\n",
    "                                        # PROXY_PASSWORD = 'password'\n",
    "                                        # PROXY_ENDPOINT = 'gate.smartproxy.com'\n",
    "                                        # PROXY_PORT = '7000'\n",
    "\n",
    "                                        # DOWNLOADER_MIDDLEWARES = { \n",
    "                                        #     'bookscraper.middlewares.MyProxyMiddleware': 350, \n",
    "                                        #     'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 400, \n",
    "                                        # }\n",
    "                                # then, create a class in 'middlewares.py' file and add this class : \n",
    "                                        import base64\n",
    "\n",
    "                                        # class MyProxyMiddleware(object):\n",
    "\n",
    "                                        #     @classmethod\n",
    "                                        #     def from_crawler(cls, crawler):\n",
    "                                        #         return cls(crawler.settings)\n",
    "\n",
    "                                        #     def __init__(self, settings):\n",
    "                                        #         self.user = settings.get('PROXY_USER')\n",
    "                                        #         self.password = settings.get('PROXY_PASSWORD')\n",
    "                                        #         self.endpoint = settings.get('PROXY_ENDPOINT')\n",
    "                                        #         self.port = settings.get('PROXY_PORT')\n",
    "\n",
    "                                        #     def process_request(self, request, spider):\n",
    "                                        #         user_credentials = '{user}:{passw}'.format(user=self.user, passw=self.password)\n",
    "                                        #         basic_authentication = 'Basic ' + base64.b64encode(user_credentials.encode()).decode()\n",
    "                                        #         host = 'http://{endpoint}:{port}'.format(endpoint=self.endpoint, port=self.port)\n",
    "                                        #         request.meta['proxy'] = host\n",
    "                                        #         request.headers['Proxy-Authorization'] = basic_authentication\n",
    "\n",
    "                        # we have implemented proxies from scrapeops.io api. see adv_spider_sub2.py \n",
    "                                #  to implement the same using middle wares, see adv_spider_sub2_pt2.py script\n",
    "                                        # for this install scrapeops-scrapy-proxy-sdk(pip install scrapeops-scrapy-proxy-sdk)\n",
    "\n",
    "                        # running spider in cloud :  refer : https://www.youtube.com/watch?v=mBoX_JCKZTE\n",
    "                            # using scrapyd\n",
    "                            # using scrapeops\n",
    "                            # using scrapy cloud\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
